---
title: "Study: Many Archived Packages Return to CRAN"
execute:
  freeze: auto
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  echo = FALSE
)
```


```{r cran-history, echo = FALSE}
# Searches packages in PACKAGES.in file and extracts relevant information.
library("dplyr")
Sys.setLanguage("en")
url <- "https://cran.r-project.org/src/contrib/PACKAGES.in"
con <- url(url)
file <- read.dcf(con) |> 
  as.data.frame()

# Extract multiline comments
comments_l <- lapply(file$`X-CRAN-Comment`, function(x) {
  trimws(unlist(strsplit(x, "[\n]+"), FALSE, FALSE))
})
comments_c <- unlist(comments_l, FALSE, FALSE)
df <- data.frame(package = rep(file$Package, lengths(comments_l)),
                 comment = comments_c)
regex_date <- "([0-9]{4}-[0-9]{2}-[0-9]{2})"
regex_action <- "([Uu]narchived?|[Aa]rchived?|[Rr]enamed?|[Oo]rphaned?|[Rr]eplaced?|[Rr]emoved?)"
comments_df <- cbind(df, 
                     strcapture(pattern = regex_date, x = df$comment, 
                                proto = data.frame(date = Sys.Date()[0])),
                     strcapture(pattern = regex_action, x = df$comment,
                                proto = data.frame(action = character()))
) |> 
  filter(!is.na(comment)) |> 
  mutate(action = tolower(action))
# Check that count(comments_df, !is.na(date), !is.na(action), sort = TRUE) makes sense
# Handle rolled and no keyword used
comments_df$action[!is.na(comments_df$date) & is.na(comments_df$action)] <- "archived"

# filter(comments_df, !is.na(action) & is.na(date)) |> count(tolower(comment)) |> View("a")
# filter(comments_df, is.na(action) & is.na(date)) |> count(tolower(comment)) |> View("b")
# Handle CRAN-history
history_l <- lapply(file$`X-CRAN-History`, function(x) {
  trimws(unlist(strsplit(x, "[\n]+"), FALSE, FALSE))
})
history_c <- unlist(history_l)

history_df <- data.frame(package = rep(file$Package, lengths(history_l)),
                         comment = history_c) |> 
  filter(!is.na(comment))

history_df <- cbind(history_df,
                    strcapture(pattern = regex_date, x = history_df$comment, 
                               proto = data.frame(date = Sys.Date()[0])),
                    strcapture(pattern = regex_action, x = history_df$comment,
                               proto = data.frame(action = character()))
) |> 
  mutate(action = tolower(action))
history_df$action[grep("Back on CRAN", history_df$comment, ignore.case = TRUE)] <- "unarchived"

full_history <- rbind(comments_df, history_df) |> 
  mutate(action = gsub(pattern = "e$", replacement = "ed", action)) |> 
  arrange(package) |>
  relocate(date) |>
  relocate(comment, .after = last_col())
# Keeping only the history with a recognized event (even if it is not interesting)
history <- filter(full_history, 
                  !is.na(action),
                  !is.na(date)) 
```

```{r search-cran-archive, echo = FALSE}
archive <- tools:::CRAN_archive_db()
pkges <- unique(history$package[history$action %in% c("archived", "unarchived")])

# packages in archive
pkgs <- intersect(pkges, names(archive))
relevant_archive <- archive[pkgs]
archive_df <- do.call(rbind, relevant_archive)

archives <- vapply(relevant_archive, nrow, numeric(1))
pkg <- rep(names(relevant_archive), times = archives)
archive_df$package <- pkg

current <- tools:::CRAN_current_db()
current_packages <- gsub("(.*)_.*\\.tar\\.gz$", "\\1", 
                                 rownames(current))
current$package <- current_packages
relevant_current <- current[current$package %in% pkgs, ]

packages <- rbind(archive_df, relevant_current) |> 
  mutate(date = as.Date(mtime), action = "accepted") |> 
  arrange(package, date) |> 
  select(date, package, action)

rownames(packages) <- NULL
```

```{r merge, echo = FALSE}
# Merge history and packages in archive
out <- merge(packages, history, all = TRUE, sort = FALSE) |> 
  arrange(package, date) 
```

CRAN packages are archived all the time, but a large portion of them
eventually gets fixed and return to CRAN.  Using public data available
from different resources[^1] on CRAN, we have found that 36% of the
archived packages get unarchived at some point [@revilla_2022]. The
median time for these packages to return to CRAN is ~33 days.

[^1]: Data sources used are `tools:::CRAN_current_db()`,
`tools:::CRAN_archive_db()`, and [PACKAGES.in].

[PACKAGES.in]: https://cran.r-project.org/src/contrib/PACKAGES.in

## Data quality checks

```{r over-unarchived, echo = FALSE}
# Packages with problems with annotation about being archived or recording the submission
over_unarchived <- out |> 
  summarise(.by = package,
            missing = sum(action == "unarchived") > sum(action == "archived")) |> 
  filter(missing) |> 
  pull(package)
```

There are `r length(over_unarchived)` packages unarchived more times than archived.
This could be because several reasons:
 - one of the previous issues with package lead to its removal. 
 - the term used for annotating an event was different (Orphaned instead of archived).

```{r start}
#| fig-cap: "**First recorded action taken of a package.** Looking by date most 
#|  packages' first action is being added to CRAN. For some it isn't."
# Packages not registered when they were first included
out |> 
  arrange(package, date) |> 
  summarise(.by = package, first_action = first(action)) |> 
  count(first_action, sort = TRUE) |> 
  knitr::kable()
no_accepted <- out |> 
  summarize(.by = package, no_accepted = any(action == "accepted")) |> 
  filter(!no_accepted) |> 
  pull(package)
```

If the first action recorded for a package is not its inclusion this could lead to problems.
This happens for `r length(no_accepted)` packages.
Maintainers of the packages might have an exchange with the CRAN volunteers that lead to unarchiving the package without new accepted packages. 
It could also show a missing entry in the CRAN archives.
A special mention to the removed action: This action is usually reserved to copyright issues and it is normal that it is the first action in record for a package. 
By contrast, it isn't expected that it would be the first action recorded for a package. 


```{r multiple-actions}
#| tbl-cap: "**Events per package in the same date.** Unarchiving usually 
#|  requires a new package accepted the same day or previously."
# Count dates that have multiple actions/events
# AMR package is archived multiple times and new packages dates match with unarchived dates
# ACEP package is archived and unarchived but there is no new entry for it
hfdd <- summarise(out, .by = c(package, date), multi = n_distinct(action))

# Merge data with itself
multiple_actions <- merge(out, hfdd, all = TRUE, sort = FALSE)
library("tidyr")
library("scales")

ma <- multiple_actions |> 
  count(multi) |> 
  rename(events = n)
multiple_actions |> 
  count(action, multi, sort = TRUE) |> 
  pivot_wider(names_from = action, values_from = n) |> 
  full_join(ma, by = join_by(multi)) |> 
  mutate(across(accepted:renamed, function(x){scales::percent(x/sum(x, na.rm = TRUE))}),
         events_percent = scales::percent(events/sum(events, na.rm = TRUE))) |> 
  select(multiple_actions = multi, events, events_percent, everything()) |> 
  knitr::kable(align = "c")

no_multiples_actions_pack <- multiple_actions |> 
  filter(action == "unarchived") |> 
  group_by(package) |> 
  count(action_not_same_date = action == "unarchived" & multi == 1L) |> 
  ungroup() |> 
  filter(action_not_same_date) |> 
  pull(package)

no_muliples_actions <- multiple_actions |> 
  filter(action == "unarchived" & multi == 1L) |> 
  nrow()
```

In total there are `r no_muliples_actions` unarchived events that doesn't have the corresponding accepted package included event (on the same date). 
This affects `r length(no_multiples_actions_pack)` packages out of `r n_distinct(out$packages)`.


```{r combined-actions}
#| tbl-cap: "**Actions that happend on the same date in a given package.** Mostly a new acceptance lead to a package being unarchived. In some ocasions other actions."
multiple_actions |> 
  filter(multi != 1) |> 
  group_by(package, date) |> 
  mutate(cg = cur_group_id()) |> 
  ungroup() |> 
  summarise(.by = cg, 
            type = paste(sort(unique(action)), sep = " ", collapse = " & ")) |> 
  count(type, sort = TRUE) |> 
  mutate(percent = scales::percent(n/sum(n))) |>
  rename(`multiple actions` = type, events = n) |> 
  knitr::kable(align = "c")
```


Those with more than 3 different actions imply multiple revisions from the CRAN team the same day. 


```{r current}
current_packages
all_packages <- unique(c(current_packages, names(archive)))
# length(all_packages)
# To account for packages removed too:
all_packages2 <- unique(c(all_packages, out$package))
# length(all_packages2)

# packages_removed <- out |> 
#   filter(action == "removed") |> 
#   distinct(package) |> 
#   pull(package)
# setdiff(packages_removed, all_packages2)
# setdiff(packages_removed, all_packages) |> length()
```


```{r qc}
individually <- length(no_multiples_actions_pack) +
  length(over_unarchived) +
  length(no_accepted)

pkg_failing_qc <- unique(c(no_multiples_actions_pack,
                      over_unarchived,
                      no_accepted))

tb <- table(table(c(no_multiples_actions_pack,
        over_unarchived,
        no_accepted)))
```

In total there are `r length(pkg_failing_qc)` different packages identified with problematic records/processing from `r n_distinct(out$pacakge)`. 
Some of them were found that had at two (`r unname(tb["2"])`) or three `r unname(tb["3"])` different issues.
Depending on which issue they might be corrected to the best of our abilities or simply discarded. 

## Analysis

```{r history-back, echo = FALSE}
library("tidyr")
times_unarchived <- out |> 
  filter(!package %in% over_unarchived) |> 
  group_by(package) |> 
  # Keep those packages archived once
  filter(cumsum(action %in% c("archived")) >= 1,
         # Removes some 5k packages that only have one remaining action.
         n() >= 2,
         sum(action %in% c("unarchived", "accepted")) >= sum(action == "archived")
         ) |>
  mutate(lead = lead(action, default = NA),
         lag = lag(action, default = NA),
         n = 1:n()) |> 
  filter((action == "archived" & lead %in% c("unarchived", "accepted")) | 
           (action %in% c("unarchived", "accepted") & lag == "archived")) |>
  # A string with the number of times that a given packages was archived (and there is a new)
  mutate(time_archived = rep(1:n(), each = 2, length.out = n())) |> 
  ungroup() |> 
  # Checking that the actions are consecutive
  group_by(package, time_archived) |> 
  filter(last(n) - first(n) == 1) |> 
  ungroup() |> 
  select(package, time_archived, action, date) |> 
  pivot_wider(names_from = action, values_from = date) |> 
  mutate(
    # Assume all the ones unarchived but without accepted are a new submission 
    # or a repeal
    accepted = if_else(is.na(accepted) & unarchived != archived, unarchived, accepted),
    archived2accepted = difftime(accepted, archived, units = "days")) |> 
  select(-unarchived)
```

```{r resubmissions-data}
if (!file.exists("20240519_cdh.RDS")) {
  cdh <- cransays::download_history()
}
saveRDS(cdh, file = "20240519_cdh.RDS")

library("lubridate", warn.conflicts	 = FALSE)
cran_submissions <- cdh |> 
  filter(!is.na(version),
         package %in% unique(pw$package)) |> 
  distinct(package, version, snapshot_time) |> 
  arrange(package, snapshot_time) |> 
  group_by(package, version) |> 
  summarise(initial_resubmission = as.Date(min(snapshot_time, na.rm = TRUE))) |> 
  ungroup()
```


```{r all-data}
archived <- out |> 
  group_by(package) |> 
  mutate(first_reacceptance = min(date[action == "accepted"])) |> 
  filter(cumsum(action == "archived") >= 1) |> 
  ungroup()
archived_resubmitted <- merge(archived, cran_submissions, all.x = TRUE) |> 
  filter(action == "archived",
         # This removes cases when submission was before archived
         is.na(initial_resubmission) | initial_resubmission >= date) |> 
  mutate(
    not_addressed = grepl("not addressed", comment, fixed = TRUE ),
    not_corrected = grepl("not corrected", comment, fixed = TRUE ),
    depends_on = grepl("depend(s|ed) on", comment),
    requires = grepl("as require[sd]", comment),
    archived = grepl("archived package", comment, fixed = TRUE),
    maintainer_address = grepl("maintainer address", comment, fixed = TRUE),
    email_bounced = grepl("email bounced", comment, fixed = TRUE),
    requested = grepl("requested", comment, fixed = TRUE),
    policy_violation = grepl("policy violation", comment, fixed = TRUE),
    # From WRE: "This should contain only (ASCII) letters, numbers and dot, 
    # have at least two characters and start with a letter and not end in a dot. "
    # Not fully compliant but close enough ( including quotations to find them)
    # gm <- gregexpr("['\\\"]([[:alnum:]\\.]+)['\\\"]", out_depends$comment)
    position_package = gregexpr("['\\\"]([[:alnum:]\\.]+)['\\\"]", comment),
    possible_packages = regmatches(comment, position_package),
    dependency_package = lapply(possible_packages, gsub, pattern = "\\\"|'", replacement = ""),
    # accepted = date[which(action %in% c("unarchived", "accepted"))],
    # FIXME find the package closest to the archived word (in the 2 cases it affects)
    # For the moment pick the first package
    dp = sapply(dependency_package, function(x) {x[1]}),
    dp = if_else(dp %in% c("README", "\\donttest"), NA, dp)
         ) |> 
  summarise(.by = c(package, date),
            initial_resubmission = min(initial_resubmission),
            maintainer_address = any(maintainer_address | email_bounced),
            not_fixed = any(not_addressed | not_corrected),
            archived_by_dep = any(depends_on | requires | archived),
            requested = any(requested),
            policy_violation = any(policy_violation),
            first_reacceptance = unique(first_reacceptance),
            # accepted = tail(accepted, 1),
            dp = unique(dp)
            ) |> 
  # Remove those submissions that are after some already archived version.
  group_by(package) |> 
  mutate(initial_resubmission = if_else(
    initial_resubmission > lead(date, default = Sys.Date()), NA, initial_resubmission)) |> 
  ungroup() 

archived_all <- merge(times_unarchived, archived_resubmitted, all = TRUE,
                      by.x = c("package", "archived"),
                      by.y = c("package", "date")) |> 
  mutate(before_registry = archived < as.Date("2020-09-12"), 
         back_on_cran = case_when(
           before_registry & !is.na(first_reacceptance) ~ "Accepted",
           before_registry &  is.na(first_reacceptance) ~ "Unknown",
          !before_registry & !is.na(first_reacceptance) ~ "Accepted",
          !before_registry &  is.na(first_reacceptance) & !is.na(initial_resubmission) ~ "Rejected",
          !before_registry &  is.na(first_reacceptance) & !is.na(initial_resubmission) ~ "Never resubmitted",
         .default = "Unkown2"),
         delay_submission = difftime(initial_resubmission, archived, units = "days"),
         delay_accepted = difftime(accepted, archived, units = "days"),
         time_archived = if_else(is.na(time_archived), 1, time_archived))
```


```{r archived-version}
rver <- rversions::r_versions() |> 
  mutate(date = as.Date(date)) |> 
  filter(endsWith(version, ".0"))

r_next <- function(rver, date) {
  w <- max(which(unique(date) > rver$date), na.rm = TRUE) +1
  if (w > length(rver$date)) {
    return(NA)
  }
  rver$date[w]
}

r_release <- function(rver, date) {
  # The next one to the one that is smaller than the date. 
  w <- max(which(unique(date) > rver$date), na.rm = TRUE)
  rver$date[w]
}

archived_all_versions <- archived_all |> 
  arrange(archived) |> 
  group_by(archived) |>  
  mutate(date_r_rel = r_release(rver, archived), 
         date_r_next = r_next(rver, archived),
         time_since_rel = difftime(archived, date_r_rel, units = "week"),
         time_before_next = difftime(archived, date_r_next, units = "week"),
         ) |> 
  ungroup() |> 
  filter(!is.na(date_r_rel))
```

### Summary of how long it takes packages to be unarchived

```{r table-times}
#| tbl-cap: "**Table with Summary statistics of time to get back to CRAN.** q1: 1st quartile; q3: 3rd quartile."
fiu <- function(x){is(x, "difftime")}
archived_all_versions |> summarise(.by = times_archived, 
                packages = n(), 
                min = min(timediff), 
                q1 = quantile(timediff, 0.25), 
                mean = mean(timediff), 
                median = median(timediff), 
                q3 = quantile(timediff, 0.75), 
                max = max(timediff)) |> 
  mutate(across(where(fiu), round)) |> 
  as.data.frame() |> 
  knitr::kable()
```


### Return time for packages archived only once in their lifetime

```{r plot-ecdf}
#| fig-cap: "Empirical distribution of the time it takes packages to get
#|  unarchived as a function of number of days since being archived on CRAN for the first time."
library("ggplot2")
pw |> 
  filter(times_archived == "1") |>
  ggplot() +
  stat_ecdf(aes(timediff)) +
  coord_cartesian(xlim =  c(0, 365), expand = FALSE) +
  scale_y_continuous(labels = scales::label_percent(), 
                     sec.axis = dup_axis(name = element_blank())) +
  scale_x_continuous(breaks = 30*1:12) +
  theme_minimal() +
  labs(title = "Time for packages to be back on CRAN for the first time",
       subtitle = "Focusing in packages in less than a year",
       y = "Percentage of packages back on CRAN",
       x = "Days till packages are back on CRAN")

```

### Return time for packages archived

```{r plot-ecdf-all}
#| fig-cap: "Empirical distribution of the time it takes packages to get
#|  unarchived as a function of number of days since being archived on CRAN."
pw |> 
  ggplot() +
  stat_ecdf(aes(timediff)) +
  coord_cartesian(xlim =  c(0, 365), expand = FALSE) +
  scale_y_continuous(labels = scales::label_percent(), 
                     sec.axis = dup_axis(name = element_blank())) +
  scale_x_continuous(breaks = 30*1:12) +
  theme_minimal() +
  labs(title = "Time for packages to be back on CRAN",
       y = "Percentage of packages back on CRAN",
       x = "Days till packages are back on CRAN")

```

### Return time for packages

```{r return-time}
first_publ <- out |> 
  filter(!package %in% no_accepted) |> 
  summarise(.by = package,
            first = min(date[action == "accepted"], na.rm =  TRUE))

publication_archived <- pw |> 
  left_join(first_publ, by = join_by(package)) |> 
  mutate(timediff2 = difftime(accepted, archived, units = "weeks"),
         time_since_first = difftime(archived, first, units = "weeks"))
library("forcats")
publication_archived |> 
  mutate(package = fct_reorder(package, archived, .fun = min)) |> 
  ggplot() +
  # geom_point(aes(archived, package), shape = "square") +
  # geom_point(aes(accepted, package)) +
  geom_segment(aes(y = package, x = accepted, xend = archived, col = -timediff2)) +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  theme_minimal() +
  theme(axis.text.y = element_blank(), 
        panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank())
publication_archived |> 
  ggplot() +
  geom_point(aes(time_since_first, timediff2, col = times_archived)) +
  labs(x = "Time from publication to archival (weeks)",
       y = "Time since archival to new accepted pacakge (weeks)",
       col = "Times archived",
       # FIXME
       title = "Newer packages are archived sooner and take longer to be fixed",
       subtitle = "Not adjusted to the number of packages published") +
  scale_x_continuous(expand = expansion(add = NA_integer_)) +
  scale_y_continuous(expand = expansion(add = c(0, NA), mult = c(0, NA))) +
  theme_minimal() +
  theme(legend.position = "inside", legend.position.inside = c(0.8, 0.7),
        legend.background = element_rect(), plot.title.position = "plot")
  
publication_archived |> 
  ggplot() +
  geom_point(aes(archived, timediff2, col = times_archived)) +
  geom_abline(intercept = Sys.Date(), slope = -2) +
  geom_rug(aes(accepted, timediff2), sides = "l") +
  scale_y_continuous(expand = expansion(mult = c(0, NA), add = c(0, NA)), 
                     sec.axis = dup_axis()) +
  theme_minimal() +
  labs(title = "Time for packages to be back on CRAN",
       y = "Time since archival (weeks)",
       x = "Date of archival",
       col = "Times archived") +
  theme(legend.position = "inside", legend.position.inside = c(0.85, 0.7),
        legend.background = element_rect(), plot.title.position = "plot")
publication_archived |> 
  ggplot() +
  geom_point(aes(first, timediff2, col = times_archived)) +
  geom_abline(intercept = Sys.Date(), slope = -2) +
  geom_rug(aes(accepted, timediff2), sides = "l") +
  scale_y_continuous(expand = expansion(mult = c(0, NA), add = c(0, NA)), 
                     sec.axis = dup_axis()) +
  theme_minimal() +
  labs(title = "Time for packages to be back on CRAN",
       y = "Time since archival (weeks)",
       x = "Date of publication",
       col = "Times archived") +
  theme(legend.position = "inside", legend.position.inside = c(0.85, 0.7),
        legend.background = element_rect(), plot.title.position = "plot")

publication_archived |> 
  ggplot() +
  geom_histogram(aes(timediff2), binwidth = 4) +
  labs(x = "Time from archival to acceptance (weeks)") +
  theme_minimal()
```


### Cumulative number of archived packages over the years

```{r plot-cumulative}
#| fig-cap: "**Packages actions done by the CRAN Team over time**.
#|  The CRAN Team may take different actions for packages currently on
#|  e.g. archived (solid red), orphaned (dotted yellow), removed
#|  (dashed green), renamed (dashed blue), and unarchived (dotted purple).
#|  Presented is the cumulative number of such events over time on the linear
#|  (left) and the logarithmic (right) scale."
library("patchwork")
p <- full_history |> 
  filter(!is.na(action), !is.na(date)) |> 
  arrange(date) |> 
  select(-comment) |> 
  group_by(action) |> 
  mutate(n = seq_len(n())) |> 
  ungroup() |> 
  ggplot() +
  geom_line(aes(date, n, col = action, linetype = action)) +
  scale_x_date(date_breaks = "2 year", date_labels = "%Y",
               expand = expansion()) +
  theme_minimal() +
  labs(x = "Date of the archive",
       y = "Total number of packages"
  )
p + scale_y_continuous(expand = expansion()) + 
  p + scale_y_log10(guide = "axis_logticks", expand = expansion(), 
                    breaks = c(1, 100, 2500, 5000, 7500, 10000)) +
  plot_annotation(
    title = "Accumulation of actions on packages",
  ) +
  plot_layout(guides = 'collect', axes = "collect") &
  theme(legend.position='bottom')
```


### Days to return versus date when archived

```{r plot-events}
#| fig-cap: "**Packages being archived and returning to CRAN.**
#|    Each data point represents when a CRAN package was archived (horizontal
#|    axis) and when it was unarchived (vertical axis).
#|    If more than one package was archive and unarchived on the same dates,
#|    the corresponding data point is presented as a larger disk.
#|    The gray dashed line is the event horizon."
pw |> 
  ggplot() +
  geom_count(aes(archived, timediff)) +
  geom_abline(slope = -1, intercept = Sys.Date(), linetype = 2, col = "gray") +
  geom_rug(aes(archived, timediff), sides = "b", outside = TRUE, length = unit(0.015, "npc"), 
           col = "gray") +
  theme_minimal() +
  coord_cartesian(clip = "off") +
  scale_y_continuous(expand = expansion(c(0, NA), c(0, NA))) +
  annotate("text", x = as.Date("2018-06-01"), y = 2700, 
           label = "Event horizon", col = "gray") +
  labs(x = "Date when the package was archived",
       y = "Time until it went back to CRAN",
       title = "Time till archived packages are back to CRAN",
       size = "Packages")
```


### Distribution of number of days for packages to return to CRAN

```{r plot-distribution}
#| fig-cap: "**Histogram of how long packages remain archived on CRAN**. 
#|  Each bar represents a week. Most packages return to CRAN within a month."
pw |> 
  ggplot() +
  geom_histogram(aes(timediff), binwidth = 7) +
  theme_minimal() +
  scale_y_continuous(expand = expansion(c(0, NA), c(0, NA))) +
  labs(y = "Packages that got back",
       x = "days",
       title = "Time till packages are back to CRAN")
```

### Packages archived over all

There have been at least `r n_distinct(out$package[out$action == "archived"])` packages archived from CRAN. 
From the total of `r length(archive)` in its whole history. 
Which results in  `r scales::percent(n_distinct(out$package[out$action == "archived"])/length(archive))` of all packages ever in CRAN got at one point archived.

```{r packages-archived}
#| fig-cap: "**Packages are archived multiple times**.
#|  Packages that got archived are sometimes back on CRAN and archived again"
packages_archived <- out |> 
  summarise(.by = package, 
            archived = sum(action == "archived"),
            unarchived = sum(action == "unarchived")) |> 
  # Clean inconsistencies on the data taking the most conservative approach
  mutate(archived = if_else(archived - unarchived < 0,  unarchived, archived)) |>
  mutate(unarchived = if_else(archived - unarchived > 1,  archived - 1, unarchived)) |> 
  filter(archived >= 1)

pa_histo <- packages_archived |> 
  ggplot() +
  geom_histogram(aes(archived), binwidth = 1) +
  scale_x_continuous(breaks = 1:6, expand = expansion()) +
  scale_y_continuous(breaks = c(0, 1:9*1000), 
                     expand = expansion(mult = c(0, NA), add = c(0, NA))) +
  theme_minimal() +
  labs(x = "Times archived", title = "Times a package has been archived")
pa_bar <- packages_archived |> 
  count(archived, name = "packages") |> 
  mutate(rel = packages / sum(packages)) |> 
  ggplot() +
  geom_col(aes("Archived packages", rel, fill = archived)) +
  scale_y_continuous(labels = scales::label_percent(), expand = expansion()) +
  scale_x_discrete(expand = expansion()) +
  theme_minimal() +
  labs(x = element_blank(), y = "Percentage", 
       title = "Percentage of archived packages", fill = "Times archived") 
pa_histo + pa_bar + 
  plot_layout(guides = 'collect') & 
  theme(plot.title.position = "plot", legend.position = "bottom")
```

Most packages are archived once. 

```{r packages-archived-unarchived}
#| fig-cap: "**Most packages are not back to CRAN after being archived**.
#|  Packages that got archived sometimes go back on CRAN."
pa <- packages_archived |> 
  count(archived, unarchived) |> 
  mutate(rel = n/sum(n)) 

pa |> 
  ggplot() +
  geom_point(aes(archived, unarchived, size = n, col = rel)) +
  scale_color_continuous(labels = scales::percent, breaks = 0.1*0:6) + 
  scale_x_continuous(breaks = 0:6) +
  scale_y_continuous(breaks = 0:6) +
  theme_minimal() +
  labs(x = "Times archived", y = "Times unarchived", col = "Percentage",
       size = "Packages",
       title = "Packages archived rarely get back to CRAN")
pa_percentage <- pa |> 
  group_by(back = !archived > unarchived) |> 
  summarise(n = sum(n)) |> 
  mutate(rel = n/sum(n)) |> 
  filter(back) |> 
  pull(rel)
```

Only `r scales::percent(pa_percentage)` gets back on CRAN.






```{r}
archived_all |> 
  summarise(.by = c(archived, back_on_cran),
            n = n()) |> 
  mutate(back_on_cran = factor(back_on_cran, c("Accepted", "Rejected", "Never resubmitted", "Unknown", "Unknown2"))) |> 
  ggplot() +
  geom_col(aes(archived, n, fill = back_on_cran)) +
  facet_zoom(x = archived > (Sys.Date() - 30), ylim = c(0, 125), 
             zoom.size = 1, horizontal = FALSE, show.area = FALSE) +
  scale_x_date(expand = expansion(), date_labels = "%Y-%m-%d") +
  scale_y_continuous(expand = expansion(), limits = c(0, 230)) +
  scale_fill_manual(values = c("Accepted" = "green", "Never resubmitted" = "blue", "Rejected" = "red")) +
  labs(y = "Events", x = "Date", title = "Packages archived",
       fill = "Resubmission process") +
  theme_bw() +
  theme(legend.direction = "horizontal", legend.position = "bottom")
```

# Slowdown due to resubmission

```{r resubmissions}
# We only have data of submissions since 2020-09-12:
# min(cdh$snapshot_time)
sample_size <- sum(archived_all$archived > as.Date("2020-09-12"))/nrow(archived_all)
archived_all |> 
  # filter(archived >= as.Date("2020-09-12")) |> 
  count(before_registry, new_submission = !is.na(initial), new_version = !is.na(accepted)) |> 
  mutate(perc = scales::percent(n/sum(n)),
         perc_submitted = n/sum(n[!before_registry & new_submission]),
         perc_submitted = if_else(!(!before_registry & new_submission), NA, perc_submitted),
         perc_submitted = scales::percent(perc_submitted)) |> 
  knitr::kable(align = "c")
```

Based on a the latest data available which is `r percent(sample_size)` of the archived packages. Most packages do not try to get back on CRAN. 
Those that try are mostly accepted, only very few don't get unarchived.

But how fast is the process of being back on CRAN?

```{r speed-resubmission}
archived_all |> 
  filter(!is.na(timediff)) |> 
  mutate(time_rel = as.numeric(timediff - delay_submission)/as.numeric(timediff),
         time_perc = scales::percent(time_rel)) |> 
  ggplot() +
  geom_histogram(aes(time_rel), bins = 50, fill = "green") +
  scale_x_continuous(labels = scales::label_percent(), expand = expansion()) +
  scale_y_continuous(expand = expansion(), sec.axis = dup_axis(name = element_blank())) +
  labs(title = "Percentage of time of resubmission",
       y = "Events", x = "Time since first resubmission to be back on CRAN") +
  theme_minimal()

# ma |> 
#   mutate(
#     package = as.factor(package)) |> 
#   pivot_longer(cols = c(archived, initial, accepted),
#                values_to = "dates", names_to = "type") |> 
#   mutate(type = factor(type, c("archived", "initial", "accepted"))) |> 
#   ggplot() +
#   geom_point(aes(dates, fct_reorder(package, dates, .fun = min), col = type), size = 1) +
#   scale_color_manual(values = c("archived" = "red", "initial" = "orange", "accepted" = "green")) +
#   theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) +
#   labs(y = element_blank())
```





### Packages not addressed in time

```{r history-failed}
not_addressed <- grepl("not addressed", out$comment, fixed = TRUE )
not_corrected <- grepl("not corrected", out$comment, fixed = TRUE )
history_failed <- out[out$package %in% out$package[not_addressed | not_corrected], ]
rownames(history_failed) <- NULL
history_failed$back[history_failed$package %in% current_packages] <- "yes"
history_failed$back[is.na(history_failed$back)] <- "no"


hf <- history_failed |> 
  filter(action != "unarchived") |> 
  filter(!package %in% over_unarchived) |> 
  group_by(package) |> 
  # Keep those packages archived once
  filter(cumsum(action == "archived") >= 1,
         # Removes some 5k packages that only have one remaining action.
         n() >= 2,
         sum(action == "accepted") >= sum(action == "archived")
         ) |> 
  filter(action == "archived")
  # mutate(lead = lead(action, default = NA),
  #        lag = lag(action, default = NA),
  #        n = 1:n()) |> 
  # filter((action == "archived" & lead == "accepted") | 
  #          (action == "accepted" & lag == "archived")) |>
  # # A string with the number of times that a given packages was archived (and there is a new)
  # mutate(times_archived = rep(1:n(), each = 2, length.out = n())) |> 
  # ungroup() |> 
  # # Checking that the actions are consecutive
  # group_by(package, times_archived) |> 
  # filter(last(n) - first(n) == 1) |> 
  # ungroup() |> 
  # select(package, times_archived, action, date) |> 
  # pivot_wider(names_from = action, values_from = date) |> 
  # mutate(timediff = difftime(accepted, archived, units = "days"))

multiple_archived <- history_failed |> 
  summarise(.by = package, archived = sum(action == "archived")) |> 
  count(mult = archived > 1) |> 
  filter(mult == TRUE) |> 
  pull(n)
```


Packages are archived because they are not addressed/corrected in time.
If we look in more detail on this packages we see there are `r length(unique(history_failed$package))` packages that failed to correct in time. Most of them where archived but some where orphaned (`r sum(history_failed$action == "orphaned")`), and some of them (`r multiple_archived`) were archived multiple times.


```{r failed-check-archived}
#| fig-cap: "**Packages archived because problems were not fixed on time are mostly back.**.
#|  Packages that got archived because maintainers couldn't fix the packages on time got back on time"
hf |> 
  ggplot() +
  geom_histogram(aes(date, fill = back), bins = 8*12) +
  scale_fill_manual(values = c("no" = "red", "yes" = "green")) +
  scale_y_continuous(expand = expansion(add = c(0, NA), mult = c(0, NA)), 
                     sec.axis = dup_axis(name = element_blank())) +
  scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m", 
               expand = expansion(add = NA_integer_, mult = 0)) +
  labs(y = "Events", x = element_blank(), fill = "Back on CRAN",
       title = "Packages with failed checks are back on CRAN") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90), 
        panel.grid.minor.x = element_blank(),
        legend.position = "inside", legend.position.inside = c(0.2, 0.75),
        legend.background = element_rect()) 
```


#### Linked to R-releases?

```{r r-versions}
archived_all_versions |> 
  ggplot() +
  geom_histogram(aes(abs(time_before_next), fill = back), col = "lightgray", bins = 54) +
  theme_minimal() +
  scale_y_continuous(expand = expansion(add = 0, mult = c(0, NA))) +
  scale_x_continuous(expand = expansion()) +
  labs(y = "Archived packages", fill = "Back on CRAN?", 
       x = "Time before next release (days)",
       title = "Packages archived with failing text before next release") +
  theme(legend.position = "inside", legend.position.inside = c(0.7, 0.89),
        plot.title.position = "plot", legend.background = element_rect()) 

archived_all_versions |> 
  ggplot() +
  geom_histogram(aes(time_since_rel, fill = back), col = "lightgray", bins = 54) +
  scale_y_continuous(expand = expansion(add = 0, mult = c(0, NA)),
                     sec.axis = dup_axis()) +
  scale_x_continuous(expand = expansion()) +
  labs(y = "Archived packages", fill = "Back on CRAN?", 
       x = "Time since previous release (days)",
       title = "Packages archived with failing text since last release") +
  theme_minimal() +
  theme(legend.position = "inside", legend.position.inside = c(0.2, 0.89),
        plot.title.position = "plot", legend.background = element_rect()) 
archived_all_versions |> 
  ggplot() +
  geom_count(aes(time_since_rel, abs(time_before_next))) +
  theme_minimal()

ggplot(archived_all_versions) +
  geom_histogram(aes(time_before_next + time_since_rel, fill = back), 
                 col = "lightgray", binwidth = 4) +
  annotate("text", x = 45, y = 210, label = "Previous release") +
  annotate("text", x = -45, y = 210, label = "Next release") +
  labs(y = "Packages", x = "Time to R release (weeks)", fill = "Back on CRAN?",
       title = "Packages archived in relation to R releases") +
  scale_x_continuous(expand = expansion()) +
  scale_y_continuous(expand = expansion(add = c(0, NA), mult = c(0, NA))) +
  theme_minimal() +
  theme(legend.position = "inside", legend.position.inside = c(0.5, 0.75),
        plot.title.position = "plot", legend.background = element_rect())
```



#### Age of archival

```{r aged}
out_failed <- out[out$package %in% history_failed$package, ]
not_addressed <- grepl("not addressed", out_failed$comment, fixed = TRUE )
not_corrected <- grepl("not corrected", out_failed$comment, fixed = TRUE )
out_failed$check[not_addressed | not_corrected] <- "yes"
out_failed$check[!(not_addressed | not_corrected)] <- "no"

of <- out_failed |> 
  filter(!package %in% pkg_failing_qc) |> 
  # Allow acceptance in the same day as archived
  arrange(package, date, action) |> 
  mutate(failed_check = as.numeric(check == "yes")) |> 
  group_by(package) |> 
  mutate(first = min(date[action == "accepted"], na.rm = TRUE),
         n_check = cumsum(failed_check) - failed_check) |> 
  # Keep only needed data (starting by 0 the groups)
  filter(n_check <= sum(failed_check) - 1) |>
  ungroup() |> 
  summarise(.by = c(package, n_check),
            first_subm = unique(first),
            previous_subm = date[max(which(action == "accepted"), na.rm = TRUE)],
            date_archived = date[max(which(check == "yes"), na.rm = TRUE)]) |> 
  ungroup() |> 
  # Fill packages archived again without an acceptance record 
  # (currently ~10 packages)
  group_by(package) |> 
  fill(previous_subm) |> 
  ungroup() |> 
  mutate(time_since_publ = difftime(date_archived, first_subm, units = "weeks"),
         time_since_prev = difftime(previous_subm, first_subm, units = "weeks"))

ggplot(of) +
  geom_count(aes(time_since_publ, time_since_prev))
ggplot(of) +
  geom_histogram(aes(time_since_publ), binwidth = 52)
ggplot(of) +
  geom_histogram(aes(time_since_prev), binwidth = 52)

of |> 
  ggplot() +
  geom_count(aes(date_archived, time_since_publ, col = time_since_prev)) +
  geom_smooth(aes(date_archived, time_since_publ))

of |> 
  ggplot() +
  geom_count(aes(date_archived, time_since_prev, col = time_since_publ)) +
  geom_smooth(aes(date_archived, time_since_prev))

of |> 
  count(archived_same_date_submission = date_archived == previous_subm) |> 
  mutate(rel = percent(n/sum(n)))
```

### Archived because depends on other packages

```{r depending-on}
out_depends <- out |> 
  group_by(package) |> 
  filter(any(grepl("depends on", comment, fixed = TRUE))) |> 
  ungroup() |> 
  mutate(
    # From WRE: "This should contain only (ASCII) letters, numbers and dot, 
    # have at least two characters and start with a letter and not end in a dot. "
    # Not fully compliant but close enough ( including quotations to find them)
    # gm <- gregexpr("['\\\"]([[:alnum:]\\.]+)['\\\"]", out_depends$comment)
    position_package = gregexpr("['\\\"]([[:alnum:]\\.]+)['\\\"]", comment),
    possible_packages = regmatches(comment, position_package),
    dependency_package = lapply(possible_packages, gsub, pattern = "\\\"|'", replacement = ""),
    # FIXME find the package closest to the archived word (in the 2 cases it affects)
    # For the moment pick the first package
    dp = sapply(dependency_package, function(x) {x[1]}),
    archived_by_dep = action == "archived" & grepl("depends on", comment, fixed = TRUE)) |> 
  select(-position_package, -possible_packages, -dependency_package, -comment) |> 
  group_by(package) |> 
  # Only keep those archiving due to dependencies
  mutate(arch_time = cumsum(archived_by_dep)) |> 
  filter(cumsum(!is.na(dp)) >= 1,
         cumsum(action == "archived") <= arch_time) |> 
  tidyr::fill(dp, .direction = "down") |> 
  group_by(package, arch_time) |> 
  reframe(back = any(action %in% c("unarchived", "accepted")),
          dp = unique(dp),
          date_back = head(date[action %in% c("unarchived", "accepted")], 1),
          date = date[action == "archived"]) |> 
  mutate(diff = difftime(date_back, date, units = "days"))

packages_archived <- out |> 
  filter(package %in% date_archived$dp) |> 
  group_by(package) |> 
  filter(cumsum(action == "archived") >= 1) |> 
  ungroup()
# Note there are some 7 packages mentioned as archived but not present in records
missing_log_packages <- out_depends |> 
  filter(!is.na(dependency_package), !dependency_package %in% out$package) |> 
  pull(dependency_package)

# FIXME: This assumes that if any in back the package is back!!
# Instead of checking if the package was ever back after being archived 
# and causing problems to other packages
out_depends$ever_back[out_depends$package %in% pw$package] <- "yes"
out_depends$ever_back[is.na(out_depends$ever_back)] <- "no"

out_depends$current_back[out_depends$package %in% current_packages] <- "yes"
out_depends$current_back[is.na(out_depends$current_back)] <- "no"


out_depends$dep_ever_back[out_depends$package_depends %in% pw$package] <- "yes"
out_depends$dep_ever_back[is.na(out_depends$dep_ever_back)] <- "no"

out_depends$dep_current_back[out_depends$package_depends %in% current_packages] <- "yes"
out_depends$dep_current_back[is.na(out_depends$dep_current_back)] <- "no"


out_depends |> 
  filter(!is.na(dependency_package)) |> 
  count(dependency_package, sort = TRUE, name = "Affected packages") |> 
  count(`Affected packages`, name = "packages")

out_depends |> 
  left_join(first_publ, by = join_by(package)) |> 
  mutate(time_archival = difftime(date, first, units = "weeks")) |> 
  filter(!is.na(time_archival)) |> 
  ggplot() +
  geom_histogram(aes(first)) +
  labs(x = "Date first submission",
       y = "Packages",
       title = "Packages archived due to dependencies.") +
  theme_minimal() +
  scale_x_date(expand = expansion(), date_breaks = "2 years", date_labels = "%Y") +
  scale_y_continuous(expand = expansion(add = c(0, NA), mult = c(0, NA)))
```


### Maintainers

#### Failing email

```{r}
sum(grepl("maintainer address", out$comment))
```
